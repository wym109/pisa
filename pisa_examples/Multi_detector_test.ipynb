{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from uncertainties import unumpy as unp\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pisa\n",
    "from pisa.core.detectors import Detectors\n",
    "from pisa.core.distribution_maker import DistributionMaker\n",
    "from pisa.core.pipeline import Pipeline\n",
    "from pisa.analysis.analysis import Analysis\n",
    "from pisa import FTYPE, ureg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define two different detectors. To make things easy (and use existing cfg files) we set up two 3y DeepCore's but call them `detector1` and `detector2`. In general the Detectors class (just like the DistributionMaker class) accepts the cfg path strings as input, but since we want to modify the Pipelines here we have to initialize them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_nu = Pipeline(\"settings/pipeline/IceCube_3y_neutrinos.cfg\")\n",
    "p1_mu = Pipeline(\"settings/pipeline/IceCube_3y_muons.cfg\")\n",
    "p1_nu.detector_name, p1_mu.detector_name = 'detector1', 'detector1'\n",
    "\n",
    "p2_nu = Pipeline(\"settings/pipeline/IceCube_3y_neutrinos.cfg\")\n",
    "p2_mu = Pipeline(\"settings/pipeline/IceCube_3y_muons.cfg\")\n",
    "p2_nu.detector_name, p2_mu.detector_name = 'detector2', 'detector2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the Detectors class. We define all free parameters except the effective area as shared parameters. So there will be two effective areas (one for each detector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detectors([p1_nu, p1_mu, p2_nu, p2_mu], shared_params=['deltam31', 'theta13', 'theta23', 'nue_numu_ratio', 'Barr_uphor_ratio', 'Barr_nu_nubar_ratio', 'delta_index', 'nutau_norm', \n",
    "                                                               'nu_nc_norm', 'opt_eff_overall', 'opt_eff_lateral', 'opt_eff_headon', 'ice_scattering', 'ice_absorption', 'atm_muon_scale'])\n",
    "# this just turns on profiling\n",
    "model.profile = True\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has a number of free parameters, that will be used in our fit to the data. aeff_scale appears two times because it is not a shared parameter. If a parameter has the same name for two detectors but is not shared, the detector name is added to the parameter for all but the first detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params.free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two distribution makers are completely similar but have a different name (as intended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distribution_makers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distribution_makers[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two pipelines are quite different, with most complexity in the neutrino pipeline, that has several `Stage`s and free parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distribution_makers[0].pipelines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distribution_makers[0].pipelines[0].stages[2].params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the muon pipleine is rather simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distribution_makers[0].pipelines[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Outputs\n",
    "\n",
    "We can get individual outputs from just one pipleine like so. This fetches outputs from the neutrino pipleine, which are 12 maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = model.distribution_makers[0].pipelines[0].get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,4, figsize=(24,10))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for m, ax in zip(maps, axes):\n",
    "    m.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in just the total expecatation from the full model (all neutrinos + muons), we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in model.get_outputs(return_sum=True):\n",
    "    o.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff plots\n",
    "\n",
    "Let's explore how a change in one of our nuisance parameters affects the expected counts per bin. Here we choose a *hole ice* parameter and move it a smidge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset all free parameters to put them back to nominal values\n",
    "model.reset_free()\n",
    "nominal = model.get_outputs(return_sum=True)\n",
    "\n",
    "# shift one parameter\n",
    "model.params.opt_eff_lateral.value = 20\n",
    "model.update_params(model.params)\n",
    "sys = model.get_outputs(return_sum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model.distribution_makers)):\n",
    "    ((nominal[i][0] - sys[i][0])/nominal[i][0]).plot(symm=True, clabel=\"rel. difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "We can load the real observed data too. This is a Pipeline with no free parameters, as the data is of course fixed. Similar to before we just use the DeepCore data twice.\n",
    "NB: When developping a new analysis you will **not** be allowed to look at the data as we do here before the box opening (c.f. *blindness*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real data\n",
    "data_maker1 = Pipeline(\"settings/pipeline/IceCube_3y_data.cfg\")\n",
    "data_maker1.detector_name = 'detector1'\n",
    "\n",
    "data_maker2 = Pipeline(\"settings/pipeline/IceCube_3y_data.cfg\")\n",
    "data_maker2.detector_name = 'detector2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_maker1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_maker2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data_maker1.get_outputs(), data_maker2.get_outputs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(20, 7))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "model.reset_free()\n",
    "nominal = model.get_outputs(return_sum=True)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i].plot(ax=ax[i,0], title=\"Data\")\n",
    "    nominal[i].plot(ax=ax[i,1], title=\"Model\")\n",
    "    (data[i] - nominal[i]).plot(ax=ax[i,2], symm=True, title=\"Diff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "For fitting we need to configure a minimizer, several standard cfgs are available, but you can also define your own.\n",
    "For the fit we need to choose a `metric`, and by default, theta23 octants, which are quasi degenerate, are fit seperately, which means two fits are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana = Analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global minimizer\n",
    "nlopt_settings = {\n",
    "    \"method\": \"nlopt\",\n",
    "    \"method_kwargs\": {\n",
    "        \"algorithm\": \"NLOPT_GN_CRS2_LM\",\n",
    "        \"ftol_abs\": 1e-3,\n",
    "        \"ftol_rel\": 1e-3,\n",
    "        # other options that can be set here: \n",
    "        # xtol_abs, xtol_rel, stopval, maxeval, maxtime\n",
    "        # after maxtime seconds, stop and return best result so far\n",
    "        \"maxtime\": 500\n",
    "    },\n",
    "    \"local_fit_kwargs\": None  # no further nesting available\n",
    "}\n",
    "\n",
    "# local minimizer\n",
    "local_fit_minuit = {\n",
    "    \"method\": \"iminuit\",\n",
    "    \"method_kwargs\": {\n",
    "        \"errors\": 0.1,\n",
    "        \"precision\": 1e-14,  # default: double precision\n",
    "        \"tol\": 1e-2,  # default: 0.1\n",
    "        \"run_simplex\": False,\n",
    "        \"run_migrad\": True\n",
    "    },\n",
    "    \"local_fit_kwargs\": None\n",
    "}\n",
    "\n",
    "# octant fit for local minimizer\n",
    "fit_octant = {\n",
    "    \"method\": \"octants\",\n",
    "    \"method_kwargs\": {\n",
    "        \"angle\": \"theta23\",\n",
    "        \"inflection_point\": 45 * ureg.degrees,\n",
    "    },\n",
    "    \"local_fit_kwargs\": local_fit_minuit\n",
    "}\n",
    "\n",
    "# complete fit routine\n",
    "fit_combi = {\n",
    "    \"method\": \"staged\",\n",
    "    \"method_kwargs\": None,\n",
    "    \"local_fit_kwargs\":[\n",
    "        nlopt_settings,\n",
    "        fit_octant,\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = ana.fit_recursively(\n",
    "            data_dist=data,\n",
    "            hypo_maker=model,\n",
    "            metric=[\"mod_chi2\", \"mod_chi2\"],\n",
    "            external_priors_penalty=None,\n",
    "            **fit_octant\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can view the bestfit parameters - the result of our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfit_params = result['params'].free\n",
    "bestfit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the model with the bestfit (make a copy here, because we don't want our bestfit params to be affected (NB: stuff is passed by reference in python))\n",
    "model.update_params(copy.deepcopy(bestfit_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how good that fit looks like. We here construct signed mod_chi2 maps by hand.\n",
    "You can see that after the fit, it improved considerably, and the distribution of chi2 values is now more uniform - not much features can be seen anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 3, figsize=(20, 14))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "bestfit = model.get_outputs(return_sum=True)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i].plot(ax=ax[2*i,0], title=\"Data\")\n",
    "    nominal[i].plot(ax=ax[2*i,1], title=\"Nominal\")\n",
    "    diff = data[i] - nominal[i]\n",
    "    (abs(diff)*diff/(nominal[i] + unp.std_devs(nominal[i].hist['total']))).plot(ax=ax[2*i,2], symm=True, title=r\"signed $\\chi^2$\", vmin=-12, vmax=12)\n",
    "\n",
    "    data[i].plot(ax=ax[2*i+1,0], title=\"Data\")\n",
    "    bestfit[i].plot(ax=ax[2*i+1,1], title=\"Bestfit\")\n",
    "    diff = data[i] - bestfit[i]\n",
    "    (abs(diff)*diff/(bestfit[i] + unp.std_devs(bestfit[i].hist['total']))).plot(ax=ax[2*i+1,2], symm=True, title=r\"signed $\\chi^2$\", vmin=-12, vmax=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking the chi2 value from the fitted model, you maybe see that it is around 113, while in the minimizer loop we saw it converged to 116. It is important to keep in mind that in the fit we had extended the metric with prior penalty terms. When we add those back we get the identical number as reported in the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('detector1')\n",
    "print(data[0].metric_total(nominal[0], 'mod_chi2'))\n",
    "print(data[0].metric_total(bestfit[0], 'mod_chi2'))\n",
    "print('-----')\n",
    "print('detector2')\n",
    "print(data[1].metric_total(nominal[1], 'mod_chi2'))\n",
    "print(data[1].metric_total(bestfit[1], 'mod_chi2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating other metrics just for fun (and just for detector1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in pisa.utils.stats.ALL_METRICS:\n",
    "    try:\n",
    "        print('%s = %.3f'%(metric, data[0].metric_total(bestfit[0], metric)))\n",
    "    except:\n",
    "        print('%s failed'%metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding two detectors and prior penalty terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].metric_total(bestfit[0], 'mod_chi2') + data[1].metric_total(bestfit[1], 'mod_chi2') + model.params.priors_penalty('mod_chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['metric_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
